{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-LErYSqvc8R"
      },
      "source": [
        "# Install & Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFjjbIJjsDn1",
        "outputId": "4f0ce152-8c4e-47c4-e16a-99c685ef0193"
      },
      "outputs": [],
      "source": [
        "!pip install torchvision\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWuUzDRoxU7B",
        "outputId": "721a5511-348f-4a47-fea4-029476f05a1e"
      },
      "outputs": [],
      "source": [
        "!wget http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/edges2shoes.tar.gz\n",
        "!tar -xvf edges2shoes.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYm7ev8Iv0AN"
      },
      "source": [
        "# Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-Kfq2C1VveA4"
      },
      "outputs": [],
      "source": [
        "class Edges2ShoesDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.images = os.listdir(root_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.images[idx])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        w, h = image.size\n",
        "        input_image = image.crop((0, 0, w//2, h))\n",
        "        target_image = image.crop((w//2, 0, w, h))\n",
        "\n",
        "        if self.transform:\n",
        "            input_image = self.transform(input_image)\n",
        "            target_image = self.transform(target_image)\n",
        "\n",
        "        return input_image, target_image\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "dataset = Edges2ShoesDataset(\"/content/edges2shoes/train\", transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQuCyaVixGR0",
        "outputId": "15a34900-6008-4dc5-cddd-a56655e0ff14"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.listdir(\"/content/edges2shoes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oP7OT570tZl"
      },
      "source": [
        "#  U-Net Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EXp_rkYY0ob8"
      },
      "outputs": [],
      "source": [
        "def down_block(in_channels, out_channels, normalize=True):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False)]\n",
        "    if normalize:\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "    layers.append(nn.LeakyReLU(0.2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def up_block(in_channels, out_channels, dropout=False):\n",
        "    layers = [\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU()\n",
        "    ]\n",
        "    if dropout:\n",
        "        layers.append(nn.Dropout(0.5))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class UNetGenerator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.down1 = down_block(3, 64, normalize=False)\n",
        "        self.down2 = down_block(64, 128)\n",
        "        self.down3 = down_block(128, 256)\n",
        "        self.down4 = down_block(256, 512)\n",
        "\n",
        "        self.up1 = up_block(512, 256)\n",
        "        self.up2 = up_block(512, 128)\n",
        "        self.up3 = up_block(256, 64)\n",
        "\n",
        "        self.final = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 3, 4, 2, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.down1(x)\n",
        "        d2 = self.down2(d1)\n",
        "        d3 = self.down3(d2)\n",
        "        d4 = self.down4(d3)\n",
        "\n",
        "        u1 = self.up1(d4)\n",
        "        u1 = torch.cat([u1, d3], dim=1)\n",
        "\n",
        "        u2 = self.up2(u1)\n",
        "        u2 = torch.cat([u2, d2], dim=1)\n",
        "\n",
        "        u3 = self.up3(u2)\n",
        "        u3 = torch.cat([u3, d1], dim=1)\n",
        "\n",
        "        return self.final(u3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-H_96Dp0x16"
      },
      "source": [
        "# PatchGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cXZLGKeG0vv4"
      },
      "outputs": [],
      "source": [
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(6, 64, 4, 2, 1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 4, 2, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(128, 256, 4, 2, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(256, 1, 4, 1, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_img, target_img):\n",
        "        x = torch.cat([input_img, target_img], dim=1)\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqSIQVJ301E7"
      },
      "source": [
        "# Initialize Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LNLU-03y00Mf"
      },
      "outputs": [],
      "source": [
        "G = UNetGenerator().to(device)\n",
        "D = PatchDiscriminator().to(device)\n",
        "\n",
        "criterion_GAN = nn.BCEWithLogitsLoss()\n",
        "criterion_L1 = nn.L1Loss()\n",
        "\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VkQOpAh06Eu"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh5KyIcY04CE",
        "outputId": "e66a2abe-2449-48c5-9c66-3bc226058410"
      },
      "outputs": [],
      "source": [
        "epochs = 30\n",
        "lambda_L1 = 100\n",
        "\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (input_img, target_img) in enumerate(dataloader):\n",
        "\n",
        "        input_img = input_img.to(device)\n",
        "        target_img = target_img.to(device)\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        fake_img = G(input_img)\n",
        "\n",
        "        real_pred = D(input_img, target_img)\n",
        "        fake_pred = D(input_img, fake_img.detach())\n",
        "\n",
        "        real_loss = criterion_GAN(real_pred, torch.ones_like(real_pred))\n",
        "        fake_loss = criterion_GAN(fake_pred, torch.zeros_like(fake_pred))\n",
        "\n",
        "        D_loss = (real_loss + fake_loss) * 0.5\n",
        "        D_loss.backward()\n",
        "        optimizer_D.step()\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        fake_pred = D(input_img, fake_img)\n",
        "\n",
        "        GAN_loss = criterion_GAN(fake_pred, torch.ones_like(fake_pred))\n",
        "        L1_loss = criterion_L1(fake_img, target_img) * lambda_L1\n",
        "\n",
        "        G_loss = GAN_loss + L1_loss\n",
        "        G_loss.backward()\n",
        "        optimizer_G.step()\n",
        "        G_losses.append(G_loss.item())\n",
        "        D_losses.append(D_loss.item())\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}]  D_loss: {D_loss.item():.4f}  G_loss: {G_loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCeVO7cN0_Jv"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "nbmO8f_P09YV",
        "outputId": "f28fee4e-8dde-49de-d05d-0df446005698"
      },
      "outputs": [],
      "source": [
        "def show_images(input_img, fake_img, target_img):\n",
        "    input_img = input_img[0].permute(1,2,0).cpu().detach().numpy()\n",
        "    fake_img = fake_img[0].permute(1,2,0).cpu().detach().numpy()\n",
        "    target_img = target_img[0].permute(1,2,0).cpu().detach().numpy()\n",
        "\n",
        "    fig, axs = plt.subplots(1,3, figsize=(12,4))\n",
        "    axs[0].imshow((input_img + 1)/2)\n",
        "    axs[0].set_title(\"Input\")\n",
        "\n",
        "    axs[1].imshow((fake_img + 1)/2)\n",
        "    axs[1].set_title(\"Generated\")\n",
        "\n",
        "    axs[2].imshow((target_img + 1)/2)\n",
        "    axs[2].set_title(\"Target\")\n",
        "\n",
        "    for ax in axs:\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "input_img, target_img = next(iter(dataloader))\n",
        "input_img = input_img.to(device)\n",
        "fake_img = G(input_img)\n",
        "\n",
        "show_images(input_img, fake_img, target_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tflO1Nzy10gg",
        "outputId": "1763572d-ad54-425b-9d7a-71f552aaad68"
      },
      "outputs": [],
      "source": [
        "input_img, target_img = next(iter(dataloader))\n",
        "input_img = input_img.to(device)\n",
        "\n",
        "G.eval()\n",
        "with torch.no_grad():\n",
        "    fake_img = G(input_img)\n",
        "\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "os.makedirs(\"saved_images\", exist_ok=True)\n",
        "\n",
        "vutils.save_image(\n",
        "    (fake_img + 1) / 2,\n",
        "    \"saved_images/generated_sample.png\"\n",
        ")\n",
        "\n",
        "print(\"Generated image saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "PG-7i73S15fz",
        "outputId": "006c2b28-306d-40ec-c80f-7c44f9f464d6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(G_losses, label=\"Generator Loss\")\n",
        "plt.plot(D_losses, label=\"Discriminator Loss\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training Loss Curve\")\n",
        "\n",
        "plt.savefig(\"saved_images/loss_curve.png\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Loss curve saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZdwrRBi1wtR",
        "outputId": "eac49766-6f64-4313-968e-86dba88f5d22"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "\n",
        "torch.save(G.state_dict(), \"saved_models/generator.pth\")\n",
        "\n",
        "torch.save(D.state_dict(), \"saved_models/discriminator.pth\")\n",
        "\n",
        "print(\"Models saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh9AO61u1Dg5"
      },
      "source": [
        "# Markdown (Comparison Section)\n",
        "\n",
        "# Performance Comparison\n",
        "\n",
        "Baseline CNN:\n",
        "- Produces blurry images\n",
        "- Optimizes only L1 / MSE loss\n",
        "\n",
        "Pix2Pix:\n",
        "- Uses adversarial + L1 loss\n",
        "- Produces sharper images\n",
        "- Better texture and realism\n",
        "\n",
        "Conclusion:\n",
        "GAN improves perceptual quality significantly."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
